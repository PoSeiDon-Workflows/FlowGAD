{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398a1e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "# Check that MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "import numpy as np\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80da2b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Example of graph classification problem \"\"\"\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import global_mean_pool as gap\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear, ModuleList, ReLU, Sequential\n",
    "import torch.nn.functional as F\n",
    "import os.path as osp\n",
    "import sys\n",
    "sys.path.append('/Users/krishnanraghavan/Documents/Projects/Poseidon/graph_nn_2')\n",
    "\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from psd_gnn.dataset import Merge_PSD_Dataset, PSD_Dataset\n",
    "\n",
    "# from psd_gnn.models.graph_classifier import GNN\n",
    "from psd_gnn.utils import process_args\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, f1_score,\n",
    "                             precision_score, recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader, ImbalancedSampler, NeighborLoader\n",
    "from torch.distributions import Gumbel as G\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "torch.manual_seed(0)\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_node_features: int,\n",
    "                 n_hidden: int,\n",
    "                 n_output: int,\n",
    "                 n_conv_blocks: int = 1 ) -> None:\n",
    "        \"\"\" Init the GNN model (new version).\n",
    "\n",
    "        Args:\n",
    "            n_node_features (int): Number of features at node level.\n",
    "            n_edge_features (int): Number of features at edge level.\n",
    "            n_hidden (int): Number of hidden dimension.\n",
    "            n_output (int): number of output dimension\n",
    "            n_conv_blocks (int): Number of\n",
    "        \"\"\"\n",
    "        # super class the class structure\n",
    "        super().__init__()\n",
    "\n",
    "        # add the ability to add one or more conv layers\n",
    "        conv_blocks = []\n",
    "\n",
    "        # ability to  add one or more conv blocks\n",
    "        for _ in range(n_conv_blocks):\n",
    "            conv_blocks += [\n",
    "                GCNConv(n_node_features, n_hidden),\n",
    "                ReLU(),\n",
    "                GCNConv(n_hidden, n_hidden),\n",
    "                ReLU(),\n",
    "            ]\n",
    "\n",
    "        # group all the conv layers\n",
    "        self.conv_layers = ModuleList(conv_blocks)\n",
    "\n",
    "\n",
    "        ## Summary statistics\n",
    "        self.summary_statistics = Sequential(\n",
    "            Linear(n_hidden, n_hidden),\n",
    "            ReLU(),\n",
    "            Linear(n_hidden, 1))\n",
    "\n",
    "        self.dist = G(torch.tensor([0.8]), torch.tensor([0.5]))\n",
    "        self.loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                edge_index: torch.Tensor,\n",
    "                batch: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Processing the GNN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input features at node level.\n",
    "            edge_index (torch.Tensor): Index pairs of vertices\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor.\n",
    "        \"\"\"\n",
    "        for layer in self.conv_layers:\n",
    "            if isinstance(layer, GCNConv):\n",
    "                x = layer(x, edge_index)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        x = gap(x, batch)\n",
    "        out   = self.summary_statistics(x)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def loss_func(self, out, y):\n",
    "        out=out.to(device)\n",
    "        loss_cross_entropy = self.loss(out, y.float())\n",
    "        sample = (self.dist.log_prob(self.dist.sample_n(32).squeeze(1)).exp() )\n",
    "        out=torch.sigmoid(out)\n",
    "        loss_extreme = torch.mean(out-sample- torch.mul(out,\\\n",
    "                       torch.log( out/(sample+1e-10) ) ) )\n",
    "        return (loss_cross_entropy+loss_extreme)\n",
    "\n",
    "\n",
    "def train(model, loader):\n",
    "    \"\"\" Train function\n",
    "\n",
    "    Args:\n",
    "        model (object): GNN model instance.\n",
    "        loader (pyg.loader.DataLoader): Data loader object.\n",
    "\n",
    "    Returns:\n",
    "        float: Training accuracy.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    out_ret = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = model.loss_func(out.float(), data.y.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        out_ret+= torch.sigmoid(out).squeeze(1).detach().numpy().tolist()\n",
    "    return total_loss / len(loader.dataset), out_ret\n",
    "           \n",
    "    \n",
    "import sklearn\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    \"\"\" Evaluation function.\n",
    "\n",
    "    Args:\n",
    "        model (object): GNN model instance.\n",
    "        loader (pyg.loader.DataLoader): Data loader object.\n",
    "\n",
    "    Returns:\n",
    "        tuple (float, list): Testing accuracy, predicted labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for data in loader:\n",
    "        data = data.to(DEVICE)\n",
    "        pred = torch.sigmoid(model(data.x, data.edge_index, data.batch))\n",
    "        pred = (pred >= torch.tensor([0.5])).to(torch.int32)\n",
    "        y_pred += pred.detach().cpu().numpy().tolist()\n",
    "        y_true += data.y.detach().cpu().numpy().tolist()\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_pred, y_true)\n",
    "    return accuracy, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37948f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psd_gnn.dataset_v2 import PSD_Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "dataset = Merge_PSD_Dataset(node_level=False, binary_labels=True).shuffle()\n",
    "\n",
    "n_graphs = len(dataset)\n",
    "train_idx, test_idx = train_test_split( np.arange(n_graphs), train_size=0.6,\\\n",
    "                                       random_state=0, shuffle=True)\n",
    "val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=0, shuffle=True)\n",
    "\n",
    "# REVIEW: imbalanced sampler\n",
    "# train_sampler = ImbalancedSampler(dataset[train_idx])\n",
    "# val_sampler = ImbalancedSampler(val_idx)\n",
    "# test_sampler = ImbalancedSampler(test_idx)\n",
    "# train_loader = DataLoader(dataset[train_idx], batch_size=args['batch_size'], sampler=train_sampler)\n",
    "# val_loader = DataLoader(dataset[val_idx], batch_size=args['batch_size'])\n",
    "# test_loader = DataLoader(dataset[test_idx], batch_size=args['batch_size'])\n",
    "\n",
    "train_loader = DataLoader(dataset[train_idx], batch_size=32)\n",
    "val_loader = DataLoader(dataset[val_idx], batch_size=32)\n",
    "test_loader = DataLoader(dataset[test_idx], batch_size=32)\n",
    "\n",
    "NUM_NODE_FEATURES = dataset.num_node_features\n",
    "NUM_OUT_FEATURES = dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "664863b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all:   0%|          | 0/500 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m train_loss, outputs \u001b[39m=\u001b[39m train(model, train_loader)\n\u001b[1;32m     13\u001b[0m train_acc, y_pred \u001b[39m=\u001b[39m test(model, train_loader)\n\u001b[1;32m     14\u001b[0m val_acc, _ \u001b[39m=\u001b[39m test(model, val_loader)\n",
      "Cell \u001b[0;32mIn[5], line 123\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m    121\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    122\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39mbatch)\n\u001b[0;32m--> 123\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mloss_func(out\u001b[39m.\u001b[39;49mfloat(), data\u001b[39m.\u001b[39;49my\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m))\n\u001b[1;32m    124\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    125\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[5], line 101\u001b[0m, in \u001b[0;36mGNN.loss_func\u001b[0;34m(self, out, y)\u001b[0m\n\u001b[1;32m     99\u001b[0m sample \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist\u001b[39m.\u001b[39mlog_prob(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist\u001b[39m.\u001b[39msample_n(\u001b[39m32\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mexp() )\n\u001b[1;32m    100\u001b[0m out\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39msigmoid(out)\n\u001b[0;32m--> 101\u001b[0m loss_extreme \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(out\u001b[39m-\u001b[39;49msample\u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mmul(out,\\\n\u001b[1;32m    102\u001b[0m                torch\u001b[39m.\u001b[39mlog( out\u001b[39m/\u001b[39m(sample\u001b[39m+\u001b[39m\u001b[39m1e-10\u001b[39m) ) ) )\n\u001b[1;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m (loss_cross_entropy\u001b[39m+\u001b[39mloss_extreme)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
     ]
    }
   ],
   "source": [
    "''' Build GNN model '''\n",
    "model = GNN(NUM_NODE_FEATURES, 64, NUM_OUT_FEATURES).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-03)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "ts_start = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "pbar = tqdm(range(500), desc=f\"all\")\n",
    "best = 0\n",
    "for e in pbar:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss, outputs = train(model, train_loader)\n",
    "    train_acc, y_pred = test(model, train_loader)\n",
    "    val_acc, _ = test(model, val_loader)\n",
    "    pbar.set_postfix({\"train_loss\": train_loss,\n",
    "                      \"train_acc\": train_acc,\n",
    "                      \"val_acc\": val_acc})\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    if e%100==0:\n",
    "        actual_Data = model.dist.sample_n( 10000 ).to(device)\n",
    "        probs = model.dist.log_prob(actual_Data).exp().squeeze(1).detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "        args = np.argsort(actual_Data.squeeze(1).detach().numpy() )\n",
    "        probs = probs[args]\n",
    "        actual_Data = actual_Data[args]\n",
    "        plt.plot(actual_Data, probs)\n",
    "        x, bins, p = plt.hist(outputs, color='orange', density=True)\n",
    "        \n",
    "        for item in p:\n",
    "            item.set_height(item.get_height()/sum(x))\n",
    "    \n",
    "        plt.xlim([0,1])\n",
    "        plt.ylim([0,1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc20336c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 227  542]\n",
      " [  85 2835]]\n",
      "(array([0, 1]), array([ 103, 1127])) real (array([0, 1]), array([251, 979]))\n",
      "graph level clf: workflow all binary True test acc 0.8276 f1 0.8993 recall 0.9673 prec 0.8403\n",
      "[[ 71 180]\n",
      " [ 32 947]]\n"
     ]
    }
   ],
   "source": [
    "## The final training metrics\n",
    "ys = []\n",
    "train_acc, y_pred = test(model, train_loader)\n",
    "for data in train_loader:\n",
    "    # ys.append(data.y.item())\n",
    "    ys += data.y.detach().cpu().numpy().tolist()\n",
    "y_true = ys\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "test_acc, y_pred = test(model, test_loader)\n",
    "\n",
    "y_true = []\n",
    "for data in test_loader:\n",
    "    y_true += data.y.detach().cpu().numpy().tolist()\n",
    "\n",
    "print(np.unique(y_pred, return_counts=True), \\\n",
    "     \"real\", np.unique(y_true, return_counts=True) )\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "prec_val = precision_score(y_true, y_pred)\n",
    "f1_val = f1_score(y_true, y_pred)\n",
    "recall_val = recall_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "print(\"graph level clf:\",\n",
    "      f\"workflow all\",\n",
    "      f\"binary True\",\n",
    "      f\"test acc {test_acc:.4f}\",\n",
    "      f\"f1 {f1_val:.4f}\",\n",
    "      f\"recall {recall_val:.4f}\",\n",
    "      f\"prec {prec_val:.4f}\",\n",
    "      )\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d70198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
