r""" A demo of autoencoder for imbalannced data.

* The data is in tabular format and imbalanced
* The autoencoder is for the anomaly data
* Augmented anomaly data is generated by the autoencoder
* Model is trained on the imbalanced and augmented data

"""

# %% Imports
import numpy as np
import torch
import torch.nn.functional as F
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, scale
from torch.optim import Adam
from tqdm import tqdm

from psd_gnn.dataset import PSD_Dataset
from psd_gnn.models.autoencoder import Autoencoder, CustomLoss
from psd_gnn.utils import eval_metrics

SEED = 0
np.random.seed(SEED)
torch.manual_seed(SEED)
np.set_printoptions(precision=3)
DEVICE = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")

# %% Read the data

dataset = PSD_Dataset("../", "1000genome_new_2022",
                      node_level=True,
                      binary_labels=True,
                      normalize=False,
                      feature_option="v2",
                      force_reprocess=True)

data = dataset[0]
Xs = data.x.numpy()
ys = data.y.numpy()

# %% Read a single job
''' start with a single job '''
job_id = 30

Xs_ = Xs.reshape(-1, dataset.num_nodes_per_graph, dataset.num_features)
ys_ = ys.reshape(-1, dataset.num_nodes_per_graph)
Xs_job = Xs_[:, job_id, :]
ys_job = ys_[:, job_id]
n_samples = ys_job.shape[0]

# %% Build the model

model = Autoencoder(D_in=dataset.num_features).to(DEVICE)
optimizer = Adam(model.parameters(), lr=5e-3, weight_decay=1e-5)

loss_mse = CustomLoss()
val_losses = []
train_losses = []
test_losses = []


# %% Train a RF model
Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs_job, ys_job, test_size=0.2, random_state=SEED)
clf = RandomForestClassifier(n_estimators=10,
                             max_depth=5,
                             oob_score=True,
                             class_weight={0: 1, 1: 1.4},
                             random_state=SEED,
                             n_jobs=-1).fit(Xs_train, ys_train)
confusion_matrix(ys_test, clf.predict(scale(Xs_test)))

# %% Augmented anomalies with autoencoder.

ys_train_1_idx = np.where(ys_train == 1)[0]
Xs_train_1 = Xs_train[ys_train_1_idx, :]

# %% Train the autoencoder with anomaly data only

Xs_1_norm = torch.from_numpy(scale(Xs_train_1)).float().to(DEVICE)
pbar = tqdm(range(1000))
for e in pbar:
    optimizer.zero_grad()
    output, mu, logvar = model(Xs_1_norm)
    loss = loss_mse(output, Xs_1_norm, mu, logvar)
    loss.backward()
    optimizer.step()
    pbar.set_postfix({"loss": loss.item()})
# %%

sigma = torch.exp(0.5 * logvar)
mu.mean(axis=0), sigma.mean(axis=0)
# %% augmented anomalies with trained autoencoder
n_samples = 40
q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))
z = q.rsample(sample_shape=torch.Size([n_samples]))

with torch.no_grad():
    aug_anomalies = model.decode(z).cpu().numpy()

Xs_train_aug = np.concatenate([scale(Xs_train), aug_anomalies], axis=0)
ys_train_aug = np.concatenate([ys_train, np.ones(n_samples)], axis=0)
# %% evaluate the model with augmented anomalies
clf_aug = RandomForestClassifier(n_estimators=10,
                                 max_depth=5,
                                 oob_score=True,
                                 random_state=SEED,
                                 n_jobs=-1).fit(Xs_train_aug, ys_train_aug)
confusion_matrix(ys_test, np.round(clf_aug.predict(scale(Xs_test))))


# %% cross validation
''' Add K-fold cross validation with the augmented data '''

kf = KFold(n_splits=5, shuffle=True, random_state=SEED)
accs, accs_aug = [], []
roc_aucs, roc_aucs_aug = [], []
for i, (train_idx, test_idx) in enumerate(kf.split(Xs_job)):
    Xs_train, ys_train, Xs_test, ys_test = Xs_job[train_idx], ys_job[train_idx], Xs_job[test_idx], ys_job[test_idx]
    Xs_train, Xs_test = scale(Xs_train), scale(Xs_test)

    clf = RandomForestClassifier(n_estimators=10,
                                 max_depth=5,
                                 oob_score=True,
                                 random_state=SEED,
                                 n_jobs=-1)
    clf.fit(Xs_train, ys_train)

    res = eval_metrics(ys_test, clf.predict(Xs_test))
    accs.append(res["acc"])
    roc_aucs.append(res["roc_auc"])

    # train the autoencoder with anomaly data only
    ys_train_1_idx = np.where(ys_train == 1)[0]
    Xs_train_1 = Xs_train[ys_train_1_idx, :]
    Xs_1_norm = torch.from_numpy(Xs_train_1).float().to(DEVICE)
    pbar = tqdm(range(1000))
    for e in pbar:
        optimizer.zero_grad()
        output, mu, logvar = model(Xs_1_norm)
        loss = loss_mse(output, Xs_1_norm, mu, logvar)
        loss.backward()
        optimizer.step()
        pbar.set_postfix({"loss": loss.item()})

    # generate augmented anomalies
    n_samples = (np.bincount(ys_train)[0] - np.bincount(ys_train)[1])
    q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))
    z = q.rsample(sample_shape=torch.Size([n_samples]))

    with torch.no_grad():
        aug_anomalies = model.decode(z).cpu().numpy()

    Xs_train_aug = np.concatenate([scale(Xs_train), aug_anomalies], axis=0)
    ys_train_aug = np.concatenate([ys_train, np.ones(n_samples)], axis=0)
    # train the model with augmented anomalies
    clf.fit(Xs_train_aug, ys_train_aug)

    # evaluate the model on original data
    res_aug = eval_metrics(ys_test, clf.predict(Xs_test))
    accs_aug.append(res_aug["acc"])
    roc_aucs_aug.append(res_aug["roc_auc"])

    # reset the autoencoder for the next fold
    for layer in model.children():
        if hasattr(layer, 'reset_parameters'):
            layer.reset_parameters()

print(f"avg roc_auc with raw data: {np.array(roc_aucs).mean():.4f}")
print(f"avg roc_auc with aug data: {np.array(roc_aucs_aug).mean():.4f}")
print(f"avg acc with raw data: {np.array(accs).mean():.4f}")
print(f"avg acc with aug data: {np.array(accs_aug).mean():.4f}")

# %%
# %%


''' For each job, we can use the same autoencoder to generate augmented anomalies.'''
kf = KFold(n_splits=5, shuffle=True, random_state=SEED)
for job_id in range(dataset.num_nodes_per_graph):
    Xs_job = Xs_[:, job_id, :]
    ys_job = ys_[:, job_id]
    accs, accs_aug = [], []
    roc_aucs, roc_aucs_aug = [], []
    if 1 not in ys_job:
        continue
    for i, (train_idx, test_idx) in enumerate(kf.split(Xs_job)):
        Xs_train, ys_train, Xs_test, ys_test = Xs_job[train_idx], ys_job[train_idx], Xs_job[test_idx], ys_job[test_idx]
        Xs_train, Xs_test = scale(Xs_train), scale(Xs_test)

        clf = RandomForestClassifier(n_estimators=10,
                                     max_depth=5,
                                     oob_score=True,
                                     random_state=SEED,
                                     n_jobs=-1)
        clf.fit(Xs_train, ys_train)

        res = eval_metrics(ys_test, clf.predict(Xs_test))
        accs.append(res["acc"])
        roc_aucs.append(res["roc_auc"])

        # train the autoencoder with anomaly data only
        ys_train_1_idx = np.where(ys_train == 1)[0]
        Xs_train_1 = Xs_train[ys_train_1_idx, :]
        Xs_1_norm = torch.from_numpy(Xs_train_1).float().to(DEVICE)
        pbar = tqdm(range(1000))
        for e in pbar:
            optimizer.zero_grad()
            output, mu, logvar = model(Xs_1_norm)
            loss = loss_mse(output, Xs_1_norm, mu, logvar)
            loss.backward()
            optimizer.step()
            pbar.set_postfix({"loss": loss.item()})

        # generate augmented anomalies
        n_samples = (np.bincount(ys_train)[0] - np.bincount(ys_train)[1])
        q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))
        z = q.rsample(sample_shape=torch.Size([n_samples]))

        with torch.no_grad():
            aug_anomalies = model.decode(z).cpu().numpy()

        Xs_train_aug = np.concatenate([scale(Xs_train), aug_anomalies], axis=0)
        ys_train_aug = np.concatenate([ys_train, np.ones(n_samples)], axis=0)
        # train the model with augmented anomalies
        clf.fit(Xs_train_aug, ys_train_aug)

        # evaluate the model on original data
        res_aug = eval_metrics(ys_test, clf.predict(Xs_test))
        accs_aug.append(res_aug["acc"])
        roc_aucs_aug.append(res_aug["roc_auc"])

        # reset the autoencoder for the next fold
        for layer in model.children():
            if hasattr(layer, 'reset_parameters'):
                layer.reset_parameters()

    print(f"job: {job_id}")
    print(f"avg roc_auc with raw data: {np.array(roc_aucs).mean():.4f}")
    print(f"avg roc_auc with aug data: {np.array(roc_aucs_aug).mean():.4f}")
    print(f"avg acc with raw data: {np.array(accs).mean():.4f}")
    print(f"avg acc with aug data: {np.array(accs_aug).mean():.4f}")
# %%

''' Train the autoencoder with mixed job anomaly data only.'''

kf = KFold(n_splits=5, shuffle=True, random_state=SEED)
accs, accs_aug = [], []
roc_aucs, roc_aucs_aug = [], []

for i, (train_idx, test_idx) in enumerate(kf.split(Xs)):
    Xs_train, ys_train, Xs_test, ys_test = Xs[train_idx], ys[train_idx], Xs[test_idx], ys[test_idx]
    Xs_train, Xs_test = scale(Xs_train), scale(Xs_test)

    clf = RandomForestClassifier(n_estimators=10,
                                 max_depth=5,
                                 oob_score=True,
                                 random_state=SEED,
                                 n_jobs=-1)
    clf.fit(Xs_train, ys_train)

    res = eval_metrics(ys_test, clf.predict(Xs_test))
    accs.append(res["acc"])
    roc_aucs.append(res["roc_auc"])

    # train the autoencoder with anomaly data only
    ys_train_1_idx = np.where(ys_train == 1)[0]
    Xs_train_1 = Xs_train[ys_train_1_idx, :]
    Xs_1_norm = torch.from_numpy(Xs_train_1).float().to(DEVICE)
    pbar = tqdm(range(1000))
    for e in pbar:
        optimizer.zero_grad()
        output, mu, logvar = model(Xs_1_norm)
        loss = loss_mse(output, Xs_1_norm, mu, logvar)
        loss.backward()
        optimizer.step()
        pbar.set_postfix({"loss": loss.item()})

    # generate augmented anomalies
    n_samples = (np.bincount(ys_train)[0] - np.bincount(ys_train)[1])
    q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))
    z = q.rsample(sample_shape=torch.Size([n_samples]))

    with torch.no_grad():
        aug_anomalies = model.decode(z).cpu().numpy()

    Xs_train_aug = np.concatenate([scale(Xs_train), aug_anomalies], axis=0)
    ys_train_aug = np.concatenate([ys_train, np.ones(n_samples)], axis=0)
    # train the model with augmented anomalies
    clf.fit(Xs_train_aug, ys_train_aug)

    # evaluate the model on original data
    res_aug = eval_metrics(ys_test, clf.predict(Xs_test))
    accs_aug.append(res_aug["acc"])
    roc_aucs_aug.append(res_aug["roc_auc"])

    # reset the autoencoder for the next fold
    for layer in model.children():
        if hasattr(layer, 'reset_parameters'):
            layer.reset_parameters()

print(f"avg roc_auc with raw data: {np.array(roc_aucs).mean():.4f}")
print(f"avg roc_auc with aug data: {np.array(roc_aucs_aug).mean():.4f}")
print(f"avg acc with raw data: {np.array(accs).mean():.4f}")
print(f"avg acc with aug data: {np.array(accs_aug).mean():.4f}")
# %%
