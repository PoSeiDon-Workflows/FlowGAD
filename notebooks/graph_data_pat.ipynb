{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f83d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035fed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrycja/python_envs/graph_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ChebConv, SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch.optim.lr_scheduler as lrs\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156e2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7f9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(flag, json_path, classes):\n",
    "    \n",
    "    counter = 0\n",
    "    edge_index = []\n",
    "    lookup = {}\n",
    "    graphs = []\n",
    "    \n",
    "    with open(json_path, \"r\") as f:\n",
    "        adjacency_list = json.load(f)\n",
    "\n",
    "        for l in adjacency_list:\n",
    "            lookup[l] = counter\n",
    "            counter+=1\n",
    "\n",
    "        for l in adjacency_list:\n",
    "            for e in adjacency_list[l]:\n",
    "                edge_index.append([lookup[l], lookup[e]])\n",
    "        for d in os.listdir(\"data\"):\n",
    "            for f in glob.glob(os.path.join(\"data\", d, flag+\"*\")):\n",
    "\n",
    "                try:\n",
    "                    graph    = {\"y\": classes[d.split(\"_\")[0]], \"edge_index\": edge_index, \"x\":[]}\n",
    "                    features = pd.read_csv(f, index_col=[0])\n",
    "                    features = features.replace('', -1, regex=True)\n",
    "\n",
    "                    for l in lookup:\n",
    "                        if l.startswith(\"create_dir_\") or l.startswith(\"cleanup_\"):\n",
    "                            new_l = l.split(\"-\")[0]\n",
    "                        else:\n",
    "                            new_l = l                            \n",
    "                        job_features = features[features.index.str.startswith(new_l)][['type', 'ready',\n",
    "                                               'submit', 'execute_start', 'execute_end', 'post_script_start',\n",
    "                                               'post_script_end', 'wms_delay', 'pre_script_delay', 'queue_delay',\n",
    "                                               'runtime', 'post_script_delay', 'stage_in_delay', 'stage_out_delay']].values.tolist()[0]\n",
    "                        \n",
    "                        if len(features[features.index.str.startswith(new_l)])<1:\n",
    "                            continue\n",
    "                        if job_features[0]=='auxiliary':\n",
    "                            job_features[0]= 0\n",
    "                        if job_features[0]=='compute':\n",
    "                            job_features[0]= 1\n",
    "                        if job_features[0]=='transfer':\n",
    "                            job_features[0]= 2\n",
    "                        job_features = [-1 if x != x else x for x in job_features]\n",
    "                        graph['x'].insert(lookup[l], job_features)\n",
    "\n",
    "                    t_list=[]\n",
    "                    for i in range(len(graph['x'])):\n",
    "                        t_list.append(graph['x'][i][1])\n",
    "                    minim = min(t_list)\n",
    "                    \n",
    "                    for i in range(len(graph['x'])):\n",
    "                        lim = graph['x'][i][1:7]\n",
    "                        lim =[ v-minim for v in lim]\n",
    "                        graph['x'][i][1:7] = lim            \n",
    "                        graphs.append(graph)\n",
    "                except:\n",
    "                    print(\"Error with the file's {} format.\".format(f))\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa4b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(flag):\n",
    "\n",
    "    classes = {\"normal\": 0}\n",
    "    counter = 1\n",
    "    json_path = \"\"\n",
    "    \n",
    "    for d in os.listdir(\"data\"):\n",
    "        d = d.split(\"_\")[0]\n",
    "        \n",
    "        if d in classes:\n",
    "            continue\n",
    "        classes[d] = counter\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "    if flag == \"nowcast-clustering-16\":\n",
    "        json_path = \"adjacency_list_dags/casa_nowcast_clustering_16.json\"\n",
    "    elif flag == \"1000genome\":\n",
    "        json_path = \"adjacency_list_dags/1000genome.json\"\n",
    "    elif flag ==\"nowcast-clustering-8\":\n",
    "        json_path = \"adjacency_list_dags/casa_nowcast_clustering_8.json\"\n",
    "    elif flag == \"wind-clustering-casa\":\n",
    "        json_path = \"adjacency_list_dags/casa_wind_clustering.json\"\n",
    "    elif flag == \"wind-noclustering-casa\":\n",
    "        json_path = \"adjacency_list_dags/casa_wind_no_clustering.json\"\n",
    "        \n",
    "    graphs = parse_data(flag, json_path, classes)\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d1371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x.float(), data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x.float(), data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "66552580",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "type_names = [ \"nowcast-clustering-16\",\"1000genome\", \"nowcast-clustering-8\",\n",
    "              \"wind-clustering-casa\",\"wind-noclustering-casa\" ]\n",
    "\n",
    "for f in type_names:\n",
    "    flag = \"nowcast-clustering-16\"\n",
    "    graphs.append(load_data(flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d26ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super(GCN, self).__init__()\n",
    "            torch.manual_seed(12345)\n",
    "            self.conv1 = SAGEConv(14, hidden_channels)\n",
    "            self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "            self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "            self.lin = Linear(hidden_channels, 4)\n",
    "\n",
    "        def forward(self, x, edge_index, batch):\n",
    "            # 1. Obtain node embeddings \n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = self.conv3(x, edge_index)\n",
    "            # 2. Readout layer\n",
    "            x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "            # 3. Apply a final classifier\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.lin(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cccc2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mod(flag, data_read=1):    \n",
    "    print(flag)\n",
    "    \n",
    "    # parse data from raw files\n",
    "    if data_read ==0:\n",
    "        graphs = load_data(flag)\n",
    "        print(len(graphs))\n",
    "        with open('graph_all_'+ str(flag) + '.pkl','wb') as f:\n",
    "            pickle.dump(graphs, f)\n",
    "    else:\n",
    "        with open('graph_all_'+ str(flag) + '.pkl','rb') as f:\n",
    "            graphs = pickle.load(f)\n",
    "    \n",
    "    y_list = []\n",
    "    for gr in graphs:\n",
    "        y_list.append(gr['y'])\n",
    "    print(min(y_list))\n",
    "    print(max(y_list))\n",
    "    print(np.unique(np.array(y_list), return_counts=True))  \n",
    "    datasets=[]\n",
    "\n",
    "    for element in graphs:\n",
    "        gx = torch.tensor(np.array(element['x']) ) \n",
    "        ge = torch.tensor(np.array(element['edge_index']) ).T\n",
    "        gy = torch.tensor(np.array(element['y']).reshape([-1]))\n",
    "\n",
    "        v_min, v_max = gx.min(), gx.max()\n",
    "        new_min, new_max = -1, 1\n",
    "        gx = (gx - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "        datasets.append( Data(x=gx, edge_index=ge, y=gy) )\n",
    "\n",
    "    dataset = datasets\n",
    "    random.shuffle(datasets)\n",
    "    train_dataset = datasets[: int(len(datasets)*0.80) ]\n",
    "    test_dataset  = datasets[int(len(datasets)*0.80):]\n",
    "    \n",
    "    print(f'Number of training graphs: {len(train_dataset)}')\n",
    "    print(f'Number of test graphs: {len(test_dataset)}')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    model = GCN(hidden_channels=64).float()\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = lrs.ExponentialLR(optimizer, gamma=0.9)\n",
    "    \n",
    "    for epoch in range(1, 200):\n",
    "        print(epoch)\n",
    "        train(train_loader, model, criterion, optimizer)\n",
    "        train_acc = test(train_loader, model)\n",
    "        test_acc  = test(test_loader, model)\n",
    "        \n",
    "        if epoch%100==0:\n",
    "            scheduler.step()\n",
    "            print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epoch%100==0:\n",
    "            torch.save(model.state_dict(), 'model_'+flag+'_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_names = [ \"nowcast-clustering-16\",\"1000genome\", \"nowcast-clustering-8\",\n",
    "              \"wind-clustering-casa\",\"wind-noclustering-casa\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466e71a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nowcast-clustering-16\n",
      "11070\n",
      "0\n",
      "3\n",
      "(array([0, 1, 2, 3]), array([2430, 2700, 3240, 2700]))\n",
      "Number of training graphs: 8856\n",
      "Number of test graphs: 2214\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "Epoch: 100, Train Acc: 0.5894, Test Acc: 0.5772\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "train_mod( \"nowcast-clustering-16\", data_read=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7cf65c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_env",
   "language": "python",
   "name": "graph_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
