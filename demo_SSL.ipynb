{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A demo of Self-Supervised Learning (SSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os.path as osp\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from pygod.metrics import (eval_average_precision, eval_precision_at_k,\n",
    "                           eval_recall_at_k, eval_roc_auc)\n",
    "from scipy.special import erf\n",
    "from scipy.stats import binom\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn.models import GraphSAGE\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Batch, Data, InMemoryDataset\n",
    "\n",
    "from psd_gnn.utils import create_dir, parse_adj\n",
    "\n",
    "from psd_gnn.dataset import PSD_Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    unif = torch.rand(*shape)\n",
    "    g = -torch.log(-torch.log(unif + eps))\n",
    "    return g \n",
    "\n",
    "def sample_gumbel_softmax(logits, temperature):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "        logits: Tensor of log probs, shape = BS x k\n",
    "        temperature = scalar\n",
    "        \n",
    "        Output: Tensor of values sampled from Gumbel softmax.\n",
    "                These will tend towards a one-hot representation in the limit of temp -> 0\n",
    "                shape = BS x k\n",
    "    \"\"\"\n",
    "    g = sample_gumbel(logits.shape)\n",
    "    h = (g + logits)/temperature\n",
    "    h_max = h.max(dim=-1, keepdim=True)[0]\n",
    "    h = h - h_max\n",
    "    cache = torch.exp(h)\n",
    "    y = cache / cache.sum(dim=-1, keepdim=True)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" utility functions and classes \"\"\"\n",
    "def validate_device(gpu):\n",
    "    \"\"\" Validate GPU device. \"\"\"\n",
    "    gpu_id = int(gpu)\n",
    "    if gpu_id >= 0 and torch.cuda.is_available() and torch.cuda.device_count() > gpu_id:\n",
    "        device = torch.device(f'cuda:{gpu_id}')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "class MinMaxNormalizeFeatures(BaseTransform):\n",
    "    r\"\"\"Min-max normalizes the attributes given in :obj:`attrs` to scale between 0 and 1.\n",
    "    (functional name: :obj:`minmax_normalize_features`).\n",
    "    Args:\n",
    "        attrs (List[str], optional): The names of attributes to normalize. Defaults to [\"x\"].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attrs: List[str] = [\"x\"],\n",
    "                 min: int = 0,\n",
    "                 max: int = 1) -> None:\n",
    "        self.attrs = attrs\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        data: Union[Data, HeteroData],\n",
    "    ) -> Union[Data, HeteroData]:\n",
    "        for store in data.stores:\n",
    "            for key, value in store.items(*self.attrs):\n",
    "                # add a small eps for nan values\n",
    "                value = value.sub(value.min(dim=0)[0]).div(value.max(dim=0)[0].sub(\n",
    "                    value.min(dim=0)[0] + 1e-10))\n",
    "                value = value * (self.max - self.min) + self.min\n",
    "                store[key] = value\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SSL and SSL_base classes \"\"\"\n",
    "class SSL(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hid_dim=64,\n",
    "                 num_layers=4,\n",
    "                 dropout=0.5,\n",
    "                 weight_decay=0.,\n",
    "                 act=F.relu,\n",
    "                 alpha=None,\n",
    "                 eta=.5,\n",
    "                 contamination=0.05,\n",
    "                 lr=5e-3,\n",
    "                 epoch=200,\n",
    "                 gpu=0,\n",
    "                 batch_size=0,\n",
    "                 num_neigh=-1,\n",
    "                 margin=.5,\n",
    "                 r=.2,\n",
    "                 m=50,\n",
    "                 k=50,\n",
    "                 f=10,\n",
    "                 K=10,\n",
    "                 N=2,\n",
    "                temperature=1,\n",
    "                verbose=False):\n",
    "\n",
    "        super(SSL, self).__init__()\n",
    "        assert 0. < contamination <= 0.5,\\\n",
    "              ValueError(f\"contamination must be in (0, 0.5], got: {contamination:.2f}\")\n",
    "\n",
    "\n",
    "        self.contamination = contamination\n",
    "        self.decision_scores_ = None\n",
    "\n",
    "        \n",
    "        # model param\n",
    "        self.hid_dim = hid_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.weight_decay = weight_decay\n",
    "        self.act = act\n",
    "        self.alpha = alpha\n",
    "        self.eta = eta\n",
    "\n",
    "\n",
    "        # training param\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.device = validate_device(gpu)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_neigh = num_neigh\n",
    "        self.margin_loss_func = torch.nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "        # other param\n",
    "        self.verbose = verbose\n",
    "        self.r = r\n",
    "        # self.m = m\n",
    "        self.k = k\n",
    "        self.f = f\n",
    "        self.model = None\n",
    "        self.N=N\n",
    "        self.K=K\n",
    "        self.temperature=temperature\n",
    "\n",
    "\n",
    "    def fit(self, data, y_true=None, temperature = 1, training=True):\n",
    "        \"\"\"\n",
    "        Fit detector with input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : PyTorch Geometric Data instance (torch_geometric.data.Data)\n",
    "            The input data.\n",
    "        y_true : numpy.ndarray, optional\n",
    "            The optional outlier ground truth labels used to monitor\n",
    "            the training progress. They are not used to optimize the\n",
    "            unsupervised model. Default: ``None``.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        \n",
    "        data.node_idx = torch.arange(data.x.shape[0])\n",
    "\n",
    "\n",
    "        # automated balancing by std\n",
    "        if self.alpha is None:\n",
    "            adj = to_dense_adj(data.edge_index)[0]\n",
    "            self.alpha = torch.std(adj).detach() / \\\n",
    "                (torch.std(data.x).detach() + torch.std(adj).detach())\n",
    "            adj = None\n",
    "\n",
    "\n",
    "        if self.batch_size == 0:\n",
    "            self.batch_size = data.x.shape[0]\n",
    "\n",
    "\n",
    "        loader = NeighborLoader(data,\n",
    "                                [self.num_neigh] * self.num_layers,\n",
    "                                batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "        self.model = SSL_Base(in_dim=data.x.shape[1],\n",
    "                                hid_dim=self.hid_dim,\n",
    "                                num_layers=self.num_layers,\n",
    "                                dropout=self.dropout,\n",
    "                                act=self.act).to(self.device)\n",
    "\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                     lr=self.lr,\n",
    "                                     weight_decay=self.weight_decay)\n",
    "\n",
    "\n",
    "        self.model.train()\n",
    "        temp_min= 0.001\n",
    "        ANNEAL_RATE = 0.0002\n",
    "\n",
    "\n",
    "        # decision scores for each node\n",
    "        decision_scores = np.zeros(data.x.shape[0])\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "\n",
    "            epoch_loss = 0\n",
    "            for sampled_data in loader:\n",
    "\n",
    "\n",
    "                batch_size = sampled_data.batch_size\n",
    "                node_idx = sampled_data.node_idx\n",
    "                x, edge_index = self.process_graph(sampled_data)\n",
    "\n",
    "\n",
    "                # generate augmented graph\n",
    "                x_aug, label_aug = self._data_augmentation(x)\n",
    "                h_aug = self.model.embed(x_aug, edge_index)\n",
    "                h = self.model.embed(x, edge_index)\n",
    "                h = F.log_softmax(h.view(-1, self.N, self.K), dim=-1)\n",
    "                h_aug = F.log_softmax(h.view(-1, self.N, self.K), dim=-1)\n",
    "                \n",
    "\n",
    "                # Sampling\n",
    "                h = sample_gumbel_softmax(h, self.temperature).view(-1, self.N*self.K)\n",
    "                h_aug = sample_gumbel_softmax(h_aug, self.temperature).view(-1, self.N*self.K)\n",
    "\n",
    "\n",
    "                # margin loss\n",
    "                margin_loss = self.margin_loss_func(h, h_aug, h) * label_aug\n",
    "                margin_loss = torch.mean(margin_loss)\n",
    "\n",
    "\n",
    "                # reconstruction loss\n",
    "                x_ = self.model.reconstruct(h, edge_index)\n",
    "                score = self.loss_func(x[:batch_size], x_[:batch_size])\n",
    "\n",
    "\n",
    "                # NEW\n",
    "                # score = self.loss_func(x, x_)\n",
    "                reconstruct_loss = torch.mean(score)\n",
    "\n",
    "\n",
    "                # total loss\n",
    "                loss = self.eta * reconstruct_loss + (1 - self.eta) * margin_loss\n",
    "                decision_scores[node_idx[:batch_size]] = score.detach().cpu().numpy()\n",
    "                epoch_loss += loss.item() * batch_size\n",
    "\n",
    "\n",
    "                # NEW\n",
    "                # decision_scores[node_idx] = score.detach().cpu().numpy()\n",
    "                # epoch_loss += loss.item() * x.shape[0]\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            if epoch % 10==0:\n",
    "                self.temperature = np.maximum(self.temperature * np.exp(-ANNEAL_RATE * epoch), temp_min)\n",
    "                print(\"New Model Temperature: {}\".format(self.temperature))\n",
    "\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"Epoch {:04d}: Loss {:.4f}\".format(epoch, epoch_loss / data.x.shape[0]), end='')\n",
    "                if y_true is not None:\n",
    "                    auc = roc_auc_score(y_true, decision_scores)\n",
    "                    top_k = eval_precision_at_k(y_true, decision_scores, k=y_true.sum())\n",
    "                    print(f\" | AUC {auc:.4f} | top_k {top_k:.4f}\", end='')\n",
    "                print()\n",
    "\n",
    "\n",
    "        self.decision_scores_ = decision_scores\n",
    "        self._process_decision_scores()\n",
    "        return self\n",
    "\n",
    "\n",
    "    \n",
    "    def decision_function(self, G):\n",
    "        \"\"\"\n",
    "        Predict raw anomaly score using the fitted detector. Outliers\n",
    "        are assigned with larger anomaly scores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G : PyTorch Geometric Data instance (torch_geometric.data.Data)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outlier_scores : numpy.ndarray\n",
    "            The anomaly score of shape :math:`N`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['model'])\n",
    "        G.node_idx = torch.arange(G.x.shape[0])\n",
    "        G.s = to_dense_adj(G.edge_index)[0]\n",
    "\n",
    "        loader = NeighborLoader(G,\n",
    "                                [self.num_neigh] * self.num_layers,\n",
    "                                batch_size=self.batch_size)\n",
    "\n",
    "        self.model.eval()\n",
    "        outlier_scores = np.zeros(G.x.shape[0])\n",
    "        for sampled_data in loader:\n",
    "            batch_size = sampled_data.batch_size\n",
    "            node_idx = sampled_data.node_idx\n",
    "\n",
    "            x, edge_index = self.process_graph(sampled_data)\n",
    "\n",
    "            x_ = self.model(x, edge_index)\n",
    "            score = self.loss_func(x[:batch_size], x_[:batch_size])\n",
    "\n",
    "            outlier_scores[node_idx[:batch_size]] = score.detach().cpu().numpy()\n",
    "        return outlier_scores\n",
    "\n",
    "    def _data_augmentation(self, x):\n",
    "        r\"\"\" Data augmentation on the input graph. Four types of pseudo anomalies will be injected:\n",
    "            Attribute, deviated\n",
    "            Attribute, disproportionate\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Attribute matrix with dim (n, d).\n",
    "\n",
    "        Returns:\n",
    "            tuple: (feat_aug, label_aug)\n",
    "                    feat_aug is the augmented attribute matrix with dim (n, d),\n",
    "                    label_aug is the pseudo anomaly label with dim (n,).\n",
    "        \"\"\"\n",
    "        rate = self.r\n",
    "        surround = self.k\n",
    "        scale_factor = self.f\n",
    "\n",
    "        feat_aug = deepcopy(x)\n",
    "        num_nodes = x.shape[0]\n",
    "        label_aug = torch.zeros(num_nodes, dtype=torch.int8)\n",
    "\n",
    "        prob = torch.rand(num_nodes)\n",
    "        label_aug[prob < rate] = 1\n",
    "\n",
    "        # deviated\n",
    "        # a mask of nodes to be deviated\n",
    "        dv_mask = torch.logical_and(rate / 2 <= prob, prob < rate * 3 / 4)\n",
    "        # randomly select surrounding nodes\n",
    "        feat_c = feat_aug[torch.randperm(num_nodes)[:surround]]\n",
    "        # calculate distance between deviated nodes and surrounding nodes\n",
    "        ds = torch.cdist(feat_aug[dv_mask], feat_c)\n",
    "        # assign the least surrounding node to deviated nodes\n",
    "        feat_aug[dv_mask] = feat_c[torch.argmax(ds, 1)]\n",
    "\n",
    "        # disproportionate\n",
    "        # a mask of nodes to be disproportionate with multiple scale factors\n",
    "        mul_mask = torch.logical_and(rate * 3 / 4 <= prob, prob < rate * 7 / 8)\n",
    "        # a mask of nodes to be disproportionate with division scale factors\n",
    "        div_mask = rate * 7 / 8 <= prob\n",
    "        # scale up or down the attribute of nodes\n",
    "        feat_aug[mul_mask] *= scale_factor\n",
    "        feat_aug[div_mask] /= scale_factor\n",
    "\n",
    "        feat_aug = feat_aug.to(self.device)\n",
    "        label_aug = label_aug.to(self.device)\n",
    "        return feat_aug, label_aug\n",
    "\n",
    "    def _data_augmentation_v2(self, x):\n",
    "        r\"\"\" Data augmentation on the input graph. Four types of pseudo anomalies will be injected:\n",
    "            Attribute, deviated\n",
    "            Attribute, disproportionate\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Attribute matrix with dim (n, d).\n",
    "\n",
    "        Returns:\n",
    "            tuple: (feat_aug, label_aug)\n",
    "                    feat_aug is the augmented attribute matrix with dim (n, d),\n",
    "                    label_aug is the pseudo anomaly label with dim (n,).\n",
    "        \"\"\"\n",
    "        rate = self.r\n",
    "        surround = self.k\n",
    "        scale_factor = self.f\n",
    "\n",
    "        feat_aug = deepcopy(x)\n",
    "        num_nodes = x.shape[0]\n",
    "        label_aug = torch.zeros(num_nodes, dtype=torch.int8)\n",
    "\n",
    "        prob = torch.rand(num_nodes)\n",
    "        # label_aug[prob < rate] = 1\n",
    "\n",
    "        # deviated\n",
    "        # a mask of nodes to be deviated\n",
    "        dv_mask = torch.logical_and(rate / 2 <= prob, prob < rate * 3 / 4)\n",
    "        # randomly select surrounding nodes\n",
    "        feat_c = feat_aug[torch.randperm(num_nodes)[:surround]]\n",
    "        # calculate distance between deviated nodes and surrounding nodes\n",
    "        ds = torch.cdist(feat_aug[dv_mask], feat_c)\n",
    "        # assign the least surrounding node to deviated nodes\n",
    "        feat_aug[dv_mask] = feat_c[torch.argmax(ds, 1)]\n",
    "        label_aug[dv_mask] = 1\n",
    "\n",
    "        # disproportionate\n",
    "        # a mask of nodes to be disproportionate with multiple scale factors\n",
    "        mul_mask = torch.logical_and(rate * 3 / 4 <= prob, prob < rate * 7 / 8)\n",
    "        # a mask of nodes to be disproportionate with division scale factors\n",
    "        div_mask = rate * 7 / 8 <= prob\n",
    "        # scale up or down the attribute of nodes\n",
    "        feat_aug[mul_mask] *= scale_factor\n",
    "        feat_aug[div_mask] /= scale_factor\n",
    "        label_aug[mul_mask] = 1\n",
    "        label_aug[div_mask] = 1\n",
    "\n",
    "        feat_aug = feat_aug.to(self.device)\n",
    "        label_aug = label_aug.to(self.device)\n",
    "        return feat_aug, label_aug\n",
    "\n",
    "    def process_graph(self, G):\n",
    "        \"\"\"\n",
    "        Process the raw PyG data object into a tuple of sub data\n",
    "        objects needed for the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G : PyTorch Geometric Data instance (torch_geometric.data.Data)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x : torch.Tensor\n",
    "            Attribute (feature) of nodes.\n",
    "        s : torch.Tensor\n",
    "            Adjacency matrix of the graph.\n",
    "        edge_index : torch.Tensor\n",
    "            Edge list of the graph.\n",
    "        \"\"\"\n",
    "        # s = to_dense_adj(G.edge_index)[0].to(self.device)\n",
    "        edge_index = G.edge_index.to(self.device)\n",
    "        x = G.x.to(self.device)\n",
    "\n",
    "        return x, edge_index\n",
    "\n",
    "    def loss_func(self, x, x_):\n",
    "        \"\"\" Loss function\n",
    "\n",
    "        :: math::\n",
    "            L = \\\\sqrt{\\\\sum_{i=1}^{n} (x_i - x_i')^2}\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Original attribute matrix with dim (n, d).\n",
    "            x_ (torch.Tensor): Reconstructed attribute matrix with dim (n, d).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Loss value.\n",
    "        \"\"\"\n",
    "        diff_attribute = torch.pow(x - x_, 2)\n",
    "        score = torch.sqrt(torch.sum(diff_attribute, 1))\n",
    "        return score\n",
    "\n",
    "    def predict(self, G, return_confidence=False):\n",
    "        \"\"\"Predict if a particular sample is an outlier or not.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G : PyTorch Geometric Data instance (torch_geometric.data.Data)\n",
    "            The input graph.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outlier_labels : numpy array of shape (n_samples,)\n",
    "            For each observation, tells whether or not\n",
    "            it should be considered as an outlier according to the\n",
    "            fitted model. 0 stands for inliers and 1 for outliers.\n",
    "\n",
    "        confidence : numpy array of shape (n_samples,).\n",
    "            Only if return_confidence is set to True.\n",
    "        \"\"\"\n",
    "\n",
    "        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])\n",
    "        pred_score = self.decision_function(G)\n",
    "        prediction = (pred_score > self.threshold_).astype(int).ravel()\n",
    "\n",
    "        if return_confidence:\n",
    "            confidence = self.predict_confidence(G)\n",
    "            return prediction, confidence\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def predict_proba(self, G, method='linear', return_confidence=False):\n",
    "        \"\"\"Predict the probability of a sample being outlier. Two approaches\n",
    "        are possible:\n",
    "\n",
    "        1. simply use Min-max conversion to linearly transform the outlier\n",
    "           scores into the range of [0,1]. The model must be\n",
    "           fitted first.\n",
    "        2. use unifying scores, see :cite:`kriegel2011interpreting`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G : PyTorch Geometric Data instance (torch_geometric.data.Data)\n",
    "            The input graph.\n",
    "\n",
    "        method : str, optional (default='linear')\n",
    "            probability conversion method. It must be one of\n",
    "            'linear' or 'unify'.\n",
    "\n",
    "        return_confidence : boolean, optional(default=False)\n",
    "            If True, also return the confidence of prediction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outlier_probability : numpy array of shape (n_samples, n_classes)\n",
    "            For each observation, tells whether\n",
    "            it should be considered as an outlier according to the\n",
    "            fitted model. Return the outlier probability, ranging\n",
    "            in [0,1]. Note it depends on the number of classes, which is by\n",
    "            default 2 classes ([proba of normal, proba of outliers]).\n",
    "        \"\"\"\n",
    "\n",
    "        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])\n",
    "        train_scores = self.decision_scores_\n",
    "\n",
    "        test_scores = self.decision_function(G)\n",
    "\n",
    "        probs = np.zeros([len(test_scores), 2])\n",
    "\n",
    "        if method == 'linear':\n",
    "            scaler = MinMaxScaler().fit(train_scores.reshape(-1, 1))\n",
    "            probs[:, 1] = scaler.transform(\n",
    "                test_scores.reshape(-1, 1)).ravel().clip(0, 1)\n",
    "            probs[:, 0] = 1 - probs[:, 1]\n",
    "\n",
    "            if return_confidence:\n",
    "                confidence = self.predict_confidence(G)\n",
    "                return probs, confidence\n",
    "\n",
    "            return probs\n",
    "\n",
    "        elif method == 'unify':\n",
    "            # turn output into probability\n",
    "            pre_erf_score = (test_scores - self._mu) / (self._sigma * np.sqrt(2))\n",
    "            erf_score = erf(pre_erf_score)\n",
    "            probs[:, 1] = erf_score.clip(0, 1).ravel()\n",
    "            probs[:, 0] = 1 - probs[:, 1]\n",
    "\n",
    "            if return_confidence:\n",
    "                confidence = self.predict_confidence(G)\n",
    "                return probs, confidence\n",
    "\n",
    "            return probs\n",
    "        else:\n",
    "            raise ValueError(method,\n",
    "                             'is not a valid probability conversion method')\n",
    "\n",
    "    def predict_confidence(self, G):\n",
    "        \"\"\"Predict the model's confidence in making the same prediction\n",
    "        under slightly different training sets.\n",
    "        See :cite:`perini2020quantifying`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G : PyTorch Geometric Data instance (torch_geometric.data.Data)\n",
    "            The input graph.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        confidence : numpy array of shape (n_samples,)\n",
    "            For each observation, tells how consistently the model would\n",
    "            make the same prediction if the training set was perturbed.\n",
    "            Return a probability, ranging in [0,1].\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])\n",
    "\n",
    "        n = len(self.decision_scores_)\n",
    "\n",
    "        # todo: this has an optimization opportunity since the scores may\n",
    "        # already be available\n",
    "        test_scores = self.decision_function(G)\n",
    "\n",
    "        count_instances = np.vectorize(\n",
    "            lambda x: np.count_nonzero(self.decision_scores_ <= x))\n",
    "        n_instances = count_instances(test_scores)\n",
    "\n",
    "        # Derive the outlier probability using Bayesian approach\n",
    "        posterior_prob = np.vectorize(lambda x: (1 + x) / (2 + n))(n_instances)\n",
    "\n",
    "        # Transform the outlier probability into a confidence value\n",
    "        confidence = np.vectorize(\n",
    "            lambda p: 1 - binom.cdf(n - int(n * self.contamination), n, p))(\n",
    "            posterior_prob)\n",
    "        prediction = (test_scores > self.threshold_).astype('int').ravel()\n",
    "        np.place(confidence, prediction == 0, 1 - confidence[prediction == 0])\n",
    "\n",
    "        return confidence\n",
    "\n",
    "    def _set_n_classes(self, y):\n",
    "        \"\"\"Set the number of classes if `y` is presented, which is not\n",
    "        expected. It could be useful for multi-class outlier detection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array of shape (n_samples,)\n",
    "            Ground truth.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        self._classes = 2  # default as binary classification\n",
    "        if y is not None:\n",
    "            check_classification_targets(y)\n",
    "            self._classes = len(np.unique(y))\n",
    "            warnings.warn(\n",
    "                \"y should not be presented in unsupervised learning.\")\n",
    "        return self\n",
    "\n",
    "    def _process_decision_scores(self):\n",
    "        \"\"\"Internal function to calculate key attributes:\n",
    "        - threshold_: used to decide the binary label\n",
    "        - labels_: binary labels of training data\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        self.threshold_ = np.percentile(self.decision_scores_, 100 * (1 - self.contamination))\n",
    "        self.labels_ = (self.decision_scores_ > self.threshold_).astype('int').ravel()\n",
    "\n",
    "        # calculate for predict_proba()\n",
    "        self._mu = np.mean(self.decision_scores_)\n",
    "        self._sigma = np.std(self.decision_scores_)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "class SSL_Base(nn.Module):\n",
    "    r\"\"\" SSL base model.\n",
    "\n",
    "    Args:\n",
    "        in_dim (int): Dimension of input features.\n",
    "        hid_dim (int): Dimension of hidden layer.\n",
    "        num_layers (int): Total number of layers, including the decoder layers and encoder layers.\n",
    "        dropout (float): The dropout rate.\n",
    "        act (str): The activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 hid_dim,\n",
    "                 num_layers,\n",
    "                 dropout=0.5,\n",
    "                 model=\"graphsage\",\n",
    "                 act=\"relu\", \n",
    "                 K = 10, \n",
    "                 N = 2):\n",
    "        super(SSL_Base, self).__init__()\n",
    "        decoder_layers = int(num_layers / 2)\n",
    "        encoder_layers = num_layers - decoder_layers\n",
    "        self.K=K\n",
    "        self.N=N\n",
    "        self.temperature = 1\n",
    "        self.shared_encoder = GraphSAGE(in_channels=in_dim,\n",
    "                                        hidden_channels=hid_dim,\n",
    "                                        num_layers=encoder_layers,\n",
    "                                        out_channels=K*N,\n",
    "                                        dropout=dropout,\n",
    "                                        act=act)\n",
    "\n",
    "        self.attr_decoder = GraphSAGE(in_channels=K*N,\n",
    "                                      hidden_channels=hid_dim,\n",
    "                                      num_layers=decoder_layers,\n",
    "                                      out_channels=in_dim,\n",
    "                                      dropout=dropout,\n",
    "                                      act=act)\n",
    "\n",
    "    def embed(self, x, edge_index):\n",
    "        h = self.shared_encoder(x, edge_index)\n",
    "        return h\n",
    "\n",
    "    def reconstruct(self, h, edge_index):\n",
    "        # decode attribute matrix\n",
    "        x_ = self.attr_decoder(h, edge_index)\n",
    "        return x_\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # encode\n",
    "        h = self.embed(x, edge_index)\n",
    "        h = F.log_softmax(h.view(-1, self.N, self.K), dim=-1)            \n",
    "        # Sampling\n",
    "        h = sample_gumbel_softmax(h, self.temperature).view(-1, self.N*self.K)\n",
    "        # reconstruct\n",
    "        x_ = self.reconstruct(h, edge_index)\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Merge_PSD_Dataset_v1(InMemoryDataset):\n",
    "    def __init__(self, root=\"./\",\n",
    "                 name=\"all\",\n",
    "                 use_node_attr=True,\n",
    "                 use_edge_attr=False,\n",
    "                 force_reprocess=False,\n",
    "                 node_level=False,\n",
    "                 binary_labels=False,\n",
    "                 normalize=True,\n",
    "                 anomaly_cat=\"all\",\n",
    "                 anomaly_level=None,\n",
    "                 transform=None,\n",
    "                 pre_transform=None,\n",
    "                 pre_filter=None) -> None:\n",
    "        self.root = root\n",
    "        self.name = \"all\"\n",
    "        self.use_node_attr = use_node_attr\n",
    "        self.use_edge_attr = use_edge_attr\n",
    "        self.force_reprocess = force_reprocess\n",
    "        self.node_level = node_level\n",
    "        self.binary_labels = binary_labels\n",
    "        self.normalize = normalize\n",
    "        self.anomaly_cat = anomaly_cat.lower()\n",
    "        self.anomaly_level = anomaly_level\n",
    "\n",
    "        workflows = [\"1000genome_new_2022\",  \"montage\"]\n",
    "        # check all data are consistent and available\n",
    "        for wf in workflows:\n",
    "            dataset = PSD_Dataset(root=self.root,\n",
    "                                  name=wf,\n",
    "                                  use_node_attr=self.use_node_attr,\n",
    "                                  use_edge_attr=self.use_edge_attr,\n",
    "                                  force_reprocess=self.force_reprocess,\n",
    "                                  node_level=self.node_level,\n",
    "                                  binary_labels=self.binary_labels,\n",
    "                                  normalize=self.normalize,\n",
    "                                  anomaly_cat=self.anomaly_cat,\n",
    "                                  anomaly_level=self.anomaly_level)\n",
    "            print(wf, len(dataset[0]))\n",
    "\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        r\"\"\"The name of the files in the :obj:`self.processed_dir` folder that\n",
    "        must be present in order to skip processing.\n",
    "\n",
    "        Returns:\n",
    "            list: List of file names.\n",
    "        \"\"\"\n",
    "        SAVED_PATH = osp.join(osp.abspath(self.root), \"processed\", self.name)\n",
    "        create_dir(SAVED_PATH)\n",
    "        return [f'{SAVED_PATH}/binary_{self.binary_labels}_node_{self.node_level}.pt']\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\" process \"\"\"\n",
    "        data_list = []\n",
    "        for wn in [\"1000genome_new_2022\",\n",
    "                   \"montage\"]:\n",
    "            wn_path = osp.join(osp.abspath(self.root), \"processed\", wn)\n",
    "            data = torch.load(f'{wn_path}/binary_{self.binary_labels}_node_{self.node_level}.pt')[0]\n",
    "            data_list.append(data)\n",
    "\n",
    "        if self.node_level:\n",
    "            data_batch = Batch.from_data_list(data_list)\n",
    "            data = Data(x=data_batch.x, edge_index=data_batch.edge_index, y=data_batch.y)\n",
    "            data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "\n",
    "            # NOTE: split the dataset into train/val/test as 60/20/20\n",
    "            idx = np.arange(data.num_nodes)\n",
    "            train_idx, test_idx = train_test_split(\n",
    "                idx, train_size=0.6, random_state=0, shuffle=True, stratify=data.y.numpy())\n",
    "            val_idx, test_idx = train_test_split(\n",
    "                test_idx, train_size=0.5, random_state=0, shuffle=True, stratify=data.y.numpy()[test_idx])\n",
    "\n",
    "            data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "            data.train_mask[train_idx] = 1\n",
    "\n",
    "            data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "            data.val_mask[val_idx] = 1\n",
    "\n",
    "            data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "            data.test_mask[test_idx] = 1\n",
    "            torch.save(self.collate([data]), self.processed_paths[0])\n",
    "        else:\n",
    "            data, slices = self.collate(data_list)\n",
    "            torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.name}({len(self)}) node_level {self.node_level} binary_labels {self.binary_labels}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000genome_new_2022\n",
      "6\n",
      "montage\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "\"\"\" load dataset \"\"\"\n",
    "workflow = [\"1000genome_new_2022\", \"montage\"]\n",
    "dat = []\n",
    "for work in workflow:\n",
    "\n",
    "    print(work)\n",
    "    ROOT = osp.join(osp.expanduser(\"~\"), \"tmp\", \"data_new\", work)\n",
    "    pre_transform = T.Compose([MinMaxNormalizeFeatures(),\n",
    "                                T.ToUndirected(),\n",
    "                                T.RandomNodeSplit(split=\"train_rest\",\n",
    "                                                    num_val=0.2,\n",
    "                                                    num_test=0.2)])\n",
    "\n",
    "    dataset = PSD_Dataset(root=ROOT,\n",
    "                            name=work,\n",
    "                            node_level=True,\n",
    "                            binary_labels=True,\n",
    "                            normalize=False,\n",
    "                            pre_transform=pre_transform)\n",
    "    \n",
    "    print(len(dataset[0]) )\n",
    "    dat.append(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000genome_new_2022 3\n",
      "montage 6\n"
     ]
    }
   ],
   "source": [
    "dataTot = Merge_PSD_Dataset_v1(root=ROOT,\n",
    "                            node_level=True,\n",
    "                            binary_labels=True,\n",
    "                            normalize=False,\n",
    "                            pre_transform=pre_transform)\n",
    "\n",
    "Tot = dataTot[0]\n",
    "\n",
    "n_epoch = 1\n",
    "n_mod = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Model with all Workflow, absolute upperbound for what we want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Model Temperature: 1.0\n",
      "Epoch 0000: Loss 0.1934 | AUC 0.4283 | top_k 0.1644\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dict = {}\n",
    "auc, ap, prec, rec = [], [], [], []\n",
    "\n",
    "for i in range(n_mod):\n",
    "    model = SSL(hid_dim=64,\n",
    "                    weight_decay=1e-5,\n",
    "                    dropout=0.5,\n",
    "                    lr=1e-3,\n",
    "                    epoch=n_epoch,\n",
    "                    gpu=0,\n",
    "                    alpha=0.5,\n",
    "                    batch_size=32,\n",
    "                    num_neigh=5,\n",
    "                    verbose=True)\n",
    "    model.fit(Tot, Tot.y)\n",
    "    score = model.decision_scores_\n",
    "\n",
    "    y = Tot.y.bool()\n",
    "    k = sum(y)\n",
    "    if np.isnan(score).any():\n",
    "        warnings.warn('contains NaN, skip one trial.')\n",
    "        # continue\n",
    "\n",
    "    auc.append(eval_roc_auc(y, score))\n",
    "    ap.append(eval_average_precision(y, score))\n",
    "    prec.append(eval_precision_at_k(y, score, k))\n",
    "    rec.append(eval_recall_at_k(y, score, k))\n",
    "\n",
    "\n",
    "print(f\"{work}\",\n",
    "f\"AUC: {np.mean(auc):.3f}±{np.std(auc):.3f} ({np.max(auc):.3f})\",\n",
    "f\"AP: {np.mean(ap):.3f}±{np.std(ap):.3f} ({np.max(ap):.3f})\",\n",
    "f\"Prec(K) {np.mean(prec):.3f}±{np.std(prec):.3f} ({np.max(prec):.3f})\",\n",
    "f\"Recall(K): {np.mean(rec):.3f}±{np.std(rec):.3f} ({np.max(rec):.3f})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Model Temperature: 1.0\n",
      "Epoch 0000: Loss 0.2967 | AUC 0.4600 | top_k 0.3082\n",
      "Epoch 0001: Loss 0.2216 | AUC 0.5740 | top_k 0.3773\n",
      "Epoch 0002: Loss 0.2054 | AUC 0.6202 | top_k 0.4307\n",
      "Epoch 0003: Loss 0.1965 | AUC 0.6354 | top_k 0.4429\n",
      "Epoch 0004: Loss 0.1847 | AUC 0.6516 | top_k 0.4536\n",
      "1000genome_new_2022 SSL             AUC: 0.652±0.000 (0.652) AP: 0.401±0.000 (0.401) Prec(K) 0.454±0.000 (0.454) Recall(K): 0.454±0.000 (0.454)\n",
      "New Model Temperature: 1.0\n",
      "Epoch 0000: Loss 0.1975 | AUC 0.4592 | top_k 0.1616\n",
      "Epoch 0001: Loss 0.1379 | AUC 0.4904 | top_k 0.1854\n",
      "Epoch 0002: Loss 0.1321 | AUC 0.4988 | top_k 0.1858\n",
      "Epoch 0003: Loss 0.1300 | AUC 0.5007 | top_k 0.1859\n",
      "Epoch 0004: Loss 0.1293 | AUC 0.5035 | top_k 0.1882\n",
      "montage SSL             AUC: 0.504±0.000 (0.504) AP: 0.199±0.000 (0.199) Prec(K) 0.188±0.000 (0.188) Recall(K): 0.188±0.000 (0.188)\n"
     ]
    }
   ],
   "source": [
    "dict = {}\n",
    "for (data, work) in zip(dat, workflow):\n",
    "    auc, ap, prec, rec = [], [], [], []\n",
    "    for _ in range(n_mod):\n",
    "        model = SSL(hid_dim=64,\n",
    "                    weight_decay=1e-5,\n",
    "                    dropout=0.5,\n",
    "                    lr=1e-3,\n",
    "                    epoch=n_epoch,\n",
    "                    gpu=0,\n",
    "                    alpha=0.5,\n",
    "                    batch_size=32,\n",
    "                    num_neigh=5,\n",
    "                    verbose=True)\n",
    "\n",
    "    \n",
    "        model.fit(data, data.y)\n",
    "        score = model.decision_scores_\n",
    "\n",
    "        y = data.y.bool()\n",
    "        k = sum(y)\n",
    "        if np.isnan(score).any():\n",
    "            warnings.warn('contains NaN, skip one trial.')\n",
    "            # continue\n",
    "\n",
    "        auc.append(eval_roc_auc(y, score))\n",
    "        ap.append(eval_average_precision(y, score))\n",
    "        prec.append(eval_precision_at_k(y, score, k))\n",
    "        rec.append(eval_recall_at_k(y, score, k))\n",
    "\n",
    "    print(f\"{work}\",\n",
    "        f\"{model.__class__.__name__:<15}\",\n",
    "        f\"AUC: {np.mean(auc):.3f}±{np.std(auc):.3f} ({np.max(auc):.3f})\",\n",
    "        f\"AP: {np.mean(ap):.3f}±{np.std(ap):.3f} ({np.max(ap):.3f})\",\n",
    "        f\"Prec(K) {np.mean(prec):.3f}±{np.std(prec):.3f} ({np.max(prec):.3f})\",\n",
    "        f\"Recall(K): {np.mean(rec):.3f}±{np.std(rec):.3f} ({np.max(rec):.3f})\")\n",
    "    dict[work] = (auc, ap, prec, rec)\n",
    "\n",
    "with open('individual.npy', 'wb') as f:\n",
    "    np.save(f, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serially Fitting the workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Model Temperature: 1.0\n",
      "Epoch 0000: Loss 0.3004 | AUC 0.4614 | top_k 0.3205\n"
     ]
    }
   ],
   "source": [
    "model = SSL(hid_dim=64,\n",
    "            weight_decay=1e-5,\n",
    "            dropout=0.5,\n",
    "            lr=1e-3,\n",
    "            epoch=n_epoch,\n",
    "            gpu=0,\n",
    "            alpha=0.5,\n",
    "            batch_size=32,\n",
    "            num_neigh=5,\n",
    "            verbose=True)\n",
    "\n",
    "auc, ap, prec, rec = [], [], [], []\n",
    "for _ in range(n_mod):\n",
    "    \n",
    "    for (data, work) in zip(dat, workflow):\n",
    "        print(work)\n",
    "        model.fit(data, data.y)\n",
    "\n",
    "        auc_, ap_, prec_, rec_ = [], [], [], []\n",
    "        for (task, _) in zip(dat, workflow):\n",
    "            score = model.decision_function(task)\n",
    "            y = task.y.bool()\n",
    "            k = sum(y)\n",
    "            if np.isnan(score).any():\n",
    "                warnings.warn('contains NaN, skip one trial.')\n",
    "                # continue\n",
    "\n",
    "            auc_.append(eval_roc_auc(y, score))\n",
    "            ap_.append(eval_average_precision(y, score))\n",
    "            prec_.append(eval_precision_at_k(y, score, k))\n",
    "            rec_.append(eval_recall_at_k(y, score, k))\n",
    "\n",
    "\n",
    "        print(f\"AUC: {np.mean(auc_):.3f}±{np.std(auc_):.3f} ({np.max(auc_):.3f})\",\n",
    "            f\"AP: {np.mean(ap_):.3f}±{np.std(ap_):.3f} ({np.max(ap_):.3f})\",\n",
    "            f\"Prec(K) {np.mean(prec_):.3f}±{np.std(prec_):.3f} ({np.max(prec_):.3f})\",\n",
    "            f\"Recall(K): {np.mean(rec_):.3f}±{np.std(rec_):.3f} ({np.max(rec_):.3f})\")\n",
    "\n",
    "    auc.append(auc_)\n",
    "    ap.append(ap_)\n",
    "    prec.append(prec_)\n",
    "    rec.append(rec_)    \n",
    "\n",
    "with open('total.npy', 'wb') as f:\n",
    "    np.save(f, (auc, ap, prec, rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
