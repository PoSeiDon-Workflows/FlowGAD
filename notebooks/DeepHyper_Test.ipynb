{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d834cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87826f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000genome\n",
      "cpu_2\n",
      "loss_0.1\n",
      "cpu_3\n",
      "hdd_60\n",
      "hdd_50\n",
      "loss_3\n",
      "loss_5\n",
      "hdd_80\n",
      "loss_0.5\n",
      "hdd_100\n",
      "hdd_90\n",
      "normal\n",
      "hdd_70\n",
      "loss_1\n",
      "casa_nowcast_clustering_16\n",
      "cpu_2\n",
      "loss_0.1\n",
      "cpu_3\n",
      "hdd_60\n",
      "hdd_50\n",
      "loss_3\n",
      "loss_5\n",
      "hdd_80\n",
      "loss_0.5\n",
      "hdd_100\n",
      "hdd_90\n",
      "normal\n",
      "hdd_70\n",
      "loss_1\n",
      "casa_nowcast_clustering_8\n",
      "cpu_2\n",
      "loss_0.1\n",
      "cpu_3\n",
      "hdd_60\n",
      "hdd_50\n",
      "loss_3\n",
      "loss_5\n",
      "hdd_80\n",
      "loss_0.5\n",
      "hdd_100\n",
      "hdd_90\n",
      "normal\n",
      "hdd_70\n",
      "loss_1\n",
      "casa_wind_clustering\n",
      "cpu_2\n",
      "loss_0.1\n",
      "cpu_3\n",
      "hdd_60\n",
      "hdd_50\n",
      "loss_3\n",
      "loss_5\n",
      "hdd_80\n",
      "loss_0.5\n",
      "hdd_100\n",
      "hdd_90\n",
      "normal\n",
      "hdd_70\n",
      "loss_1\n",
      "casa_wind_no_clustering\n",
      "cpu_2\n",
      "loss_0.1\n",
      "cpu_3\n",
      "hdd_60\n",
      "hdd_50\n",
      "loss_3\n",
      "loss_5\n",
      "hdd_80\n",
      "loss_0.5\n",
      "hdd_100\n",
      "hdd_90\n",
      "normal\n",
      "hdd_70\n",
      "loss_1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'flag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graphs\n\u001b[1;32m    271\u001b[0m graphs\u001b[38;5;241m=\u001b[39m load_data()\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_all_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[43mflag\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    273\u001b[0m             pickle\u001b[38;5;241m.\u001b[39mdump(graphs, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'flag' is not defined"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    import json\n",
    "    import glob\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    graphs = []\n",
    "    classes = {\"normal\": 0}\n",
    "    counter = 1\n",
    "    for d in os.listdir(\"data\"):\n",
    "        d = d.split(\"_\")[0]\n",
    "        if d in classes: continue\n",
    "        classes[d] = counter\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "    print(\"1000genome\")\n",
    "    counter = 0\n",
    "    edge_index = []\n",
    "    lookup = {}\n",
    "    with open(\"adjacency_list_dags/1000genome.json\", \"r\") as f:\n",
    "        adjacency_list = json.load(f)\n",
    "    for l in adjacency_list:\n",
    "        lookup[l] = counter\n",
    "        counter+=1\n",
    "    for l in adjacency_list:\n",
    "        for e in adjacency_list[l]:\n",
    "            edge_index.append([lookup[l], lookup[e]])\n",
    "    import math\n",
    "    for d in os.listdir(\"data\"):\n",
    "        print(d)\n",
    "        for f in glob.glob(os.path.join(\"data\", d, \"1000genome*\")):\n",
    "            graph = {\"y\": classes[d.split(\"_\")[0]], \"edge_index\": edge_index, \"x\":[]}\n",
    "            features = pd.read_csv(f, index_col=[0])\n",
    "            features= features.replace('', -1, regex=True)\n",
    "            # time_list\n",
    "            for l in lookup:\n",
    "                if l.startswith(\"create_dir_\") or l.startswith(\"cleanup_\"):\n",
    "                    new_l = l.split(\"-\")[0]\n",
    "                else:\n",
    "                    new_l = l\n",
    "                job_features = features[features.index.str.startswith(new_l)][['type', 'ready',\n",
    "                                       'submit', 'execute_start', 'execute_end', 'post_script_start',\n",
    "                                       'post_script_end', 'wms_delay', 'pre_script_delay', 'queue_delay',\n",
    "                                       'runtime', 'post_script_delay', 'stage_in_delay', 'stage_out_delay']].values.tolist()[0]\n",
    "\n",
    "                if job_features[0]=='auxiliary':\n",
    "                    job_features[0]= 0\n",
    "                if job_features[0]=='compute':\n",
    "                    job_features[0]= 1\n",
    "                if job_features[0]=='transfer':\n",
    "                    job_features[0]= 2\n",
    "\n",
    "                # print(job_features)\n",
    "                job_features = [-1 if x != x else x for x in job_features]\n",
    "                graph['x'].insert(lookup[l], job_features)\n",
    "            t_list=[]\n",
    "            for i in range(len(graph['x'])):\n",
    "                t_list.append(graph['x'][i][1])\n",
    "            minim= min(t_list)\n",
    "            for i in range(len(graph['x'])):\n",
    "                lim = graph['x'][i][1:7]\n",
    "                lim=[ v-minim for v in lim]\n",
    "                graph['x'][i][1:7]= lim\n",
    "            graphs.append(graph)\n",
    "\n",
    "\n",
    "    print(\"casa_nowcast_clustering_16\")\n",
    "    counter = 0\n",
    "    edge_index = []\n",
    "    lookup = {}\n",
    "    with open(\"adjacency_list_dags/casa_nowcast_clustering_16.json\", \"r\") as f:\n",
    "        adjacency_list = json.load(f)\n",
    "    for l in adjacency_list:\n",
    "        lookup[l] = counter\n",
    "        counter+=1\n",
    "    for l in adjacency_list:\n",
    "        for e in adjacency_list[l]:\n",
    "            edge_index.append([lookup[l], lookup[e]])               \n",
    "    for d in os.listdir(\"data\"):\n",
    "        print(d)\n",
    "        for f in glob.glob(os.path.join(\"data\", d, \"nowcast-clustering-16*\")):\n",
    "            graph = {\"y\": classes[d.split(\"_\")[0]], \"edge_index\": edge_index, \"x\":[]}\n",
    "            features = pd.read_csv(f, index_col=[0])\n",
    "            features= features.replace('', -1, regex=True)\n",
    "            # time_list\n",
    "            for l in lookup:\n",
    "                if l.startswith(\"create_dir_\") or l.startswith(\"cleanup_\"):\n",
    "                    new_l = l.split(\"-\")[0]\n",
    "                else:\n",
    "                    new_l = l\n",
    "                job_features = features[features.index.str.startswith(new_l)][['type', 'ready',\n",
    "                                       'submit', 'execute_start', 'execute_end', 'post_script_start',\n",
    "                                       'post_script_end', 'wms_delay', 'pre_script_delay', 'queue_delay',\n",
    "                                       'runtime', 'post_script_delay', 'stage_in_delay', 'stage_out_delay']].values.tolist()[0]\n",
    "                # print(job_features)\n",
    "                if job_features[0]=='auxiliary':\n",
    "                    job_features[0]= 0\n",
    "                if job_features[0]=='compute':\n",
    "                    job_features[0]= 1\n",
    "                if job_features[0]=='transfer':\n",
    "                    job_features[0]= 2\n",
    "                    #             print(job_features)\n",
    "                job_features = [-1 if x != x else x for x in job_features]\n",
    "                graph['x'].insert(lookup[l], job_features)\n",
    "            t_list=[]\n",
    "            for i in range(len(graph['x'])):\n",
    "                t_list.append(graph['x'][i][1])\n",
    "            minim= min(t_list)\n",
    "            for i in range(len(graph['x'])):\n",
    "                lim = graph['x'][i][1:7]\n",
    "                lim=[ v-minim for v in lim]\n",
    "                graph['x'][i][1:7]= lim            \n",
    "            graphs.append(graph)\n",
    "\n",
    "\n",
    "    print(\"casa_nowcast_clustering_8\")\n",
    "    counter = 0\n",
    "    edge_index = []\n",
    "    lookup = {}\n",
    "    with open(\"adjacency_list_dags/casa_nowcast_clustering_8.json\", \"r\") as f:\n",
    "        adjacency_list = json.load(f)\n",
    "    for l in adjacency_list:\n",
    "        lookup[l] = counter\n",
    "        counter+=1\n",
    "    for l in adjacency_list:\n",
    "        for e in adjacency_list[l]:\n",
    "            edge_index.append([lookup[l], lookup[e]])     \n",
    "    for d in os.listdir(\"data\"):\n",
    "        print(d)\n",
    "        for f in glob.glob(os.path.join(\"data\", d, \"nowcast-clustering-8*\")):\n",
    "            graph = {\"y\": classes[d.split(\"_\")[0]], \"edge_index\": edge_index, \"x\":[]}\n",
    "            features = pd.read_csv(f, index_col=[0])\n",
    "            features= features.replace('', -1, regex=True)\n",
    "            # time_list\n",
    "            for l in lookup:\n",
    "                if l.startswith(\"create_dir_\") or l.startswith(\"cleanup_\"):\n",
    "                    new_l = l.split(\"-\")[0]\n",
    "                else:\n",
    "                    new_l = l\n",
    "                job_features = features[features.index.str.startswith(new_l)][['type', 'ready',\n",
    "                                       'submit', 'execute_start', 'execute_end', 'post_script_start',\n",
    "                                       'post_script_end', 'wms_delay', 'pre_script_delay', 'queue_delay',\n",
    "                                       'runtime', 'post_script_delay', 'stage_in_delay', 'stage_out_delay']].values.tolist()[0]\n",
    "                if job_features[0]=='auxiliary':\n",
    "                    job_features[0]= 0\n",
    "                if job_features[0]=='compute':\n",
    "                    job_features[0]= 1\n",
    "                if job_features[0]=='transfer':\n",
    "                    job_features[0]= 2\n",
    "                    #             print(job_features)\n",
    "                job_features = [-1 if x != x else x for x in job_features]\n",
    "                graph['x'].insert(lookup[l], job_features)\n",
    "            t_list=[]\n",
    "            for i in range(len(graph['x'])):\n",
    "                t_list.append(graph['x'][i][1])\n",
    "            minim= min(t_list)\n",
    "            for i in range(len(graph['x'])):\n",
    "                lim = graph['x'][i][1:7]\n",
    "                lim=[ v-minim for v in lim]\n",
    "                graph['x'][i][1:7]= lim\n",
    "            graphs.append(graph)\n",
    "\n",
    "\n",
    "    print(\"casa_wind_clustering\")\n",
    "    counter = 0\n",
    "    edge_index = []\n",
    "    lookup = {}\n",
    "    with open(\"adjacency_list_dags/casa_wind_clustering.json\", \"r\") as f:\n",
    "        adjacency_list = json.load(f)\n",
    "    for l in adjacency_list:\n",
    "        lookup[l] = counter\n",
    "        counter+=1\n",
    "    for l in adjacency_list:\n",
    "        for e in adjacency_list[l]:\n",
    "            edge_index.append([lookup[l], lookup[e]])   \n",
    "    for d in os.listdir(\"data\"):\n",
    "        print(d)\n",
    "        for f in glob.glob(os.path.join(\"data\", d, \"wind-clustering-casa*\")):\n",
    "            graph = {\"y\": classes[d.split(\"_\")[0]], \"edge_index\": edge_index, \"x\":[]}\n",
    "            features = pd.read_csv(f, index_col=[0])\n",
    "            features= features.replace('', -1, regex=True)\n",
    "            # time_list\n",
    "            for l in lookup:\n",
    "                if l.startswith(\"create_dir_\") or l.startswith(\"cleanup_\"):\n",
    "                    new_l = l.split(\"-\")[0]\n",
    "                else:\n",
    "                    new_l = l\n",
    "                job_features = features[features.index.str.startswith(new_l)][['type', 'ready',\n",
    "                                       'submit', 'execute_start', 'execute_end', 'post_script_start',\n",
    "                                       'post_script_end', 'wms_delay', 'pre_script_delay', 'queue_delay',\n",
    "                                       'runtime', 'post_script_delay', 'stage_in_delay', 'stage_out_delay']].values.tolist()[0]\n",
    "                if job_features[0]=='auxiliary':\n",
    "                    job_features[0]= 0\n",
    "                if job_features[0]=='compute':\n",
    "                    job_features[0]= 1\n",
    "                if job_features[0]=='transfer':\n",
    "                    job_features[0]= 2\n",
    "                    #             print(job_features)\n",
    "                job_features = [-1 if x != x else x for x in job_features]\n",
    "                graph['x'].insert(lookup[l], job_features)\n",
    "            t_list=[]\n",
    "            for i in range(len(graph['x'])):\n",
    "                t_list.append(graph['x'][i][1])\n",
    "            minim= min(t_list)\n",
    "            for i in range(len(graph['x'])):\n",
    "                lim = graph['x'][i][1:7]\n",
    "                lim=[ v-minim for v in lim]\n",
    "                graph['x'][i][1:7]= lim\n",
    "            graphs.append(graph)\n",
    "\n",
    "\n",
    "    print(\"casa_wind_no_clustering\")\n",
    "    counter = 0\n",
    "    edge_index = []\n",
    "    lookup = {}\n",
    "    with open(\"adjacency_list_dags/casa_wind_no_clustering.json\", \"r\") as f:\n",
    "        adjacency_list = json.load(f)\n",
    "    for l in adjacency_list:\n",
    "        lookup[l] = counter\n",
    "        counter+=1\n",
    "    for l in adjacency_list:\n",
    "        for e in adjacency_list[l]:\n",
    "            edge_index.append([lookup[l], lookup[e]])     \n",
    "    for d in os.listdir(\"data\"):\n",
    "        print(d)\n",
    "        for f in glob.glob(os.path.join(\"data\", d, \"wind-noclustering-casa*\")):\n",
    "            if \"-20200817T052029Z-\" in f: continue\n",
    "            graph = {\"y\": classes[d.split(\"_\")[0]], \"edge_index\": edge_index, \"x\":[]}\n",
    "            features = pd.read_csv(f, index_col=[0])\n",
    "            features= features.replace('', -1, regex=True)\n",
    "            # time_list\n",
    "            for l in lookup:\n",
    "                if l.startswith(\"create_dir_\") or l.startswith(\"cleanup_\"):\n",
    "                    new_l = l.split(\"-\")[0]\n",
    "                else:\n",
    "                    new_l = l\n",
    "                # print(len(features[features.index.str.startswith(new_l)]) )\n",
    "                #type,ready,pre_script_start,pre_script_end,submit,execute_start,\n",
    "                #execute_end,post_script_start,post_script_end,wms_delay,pre_script_delay,\n",
    "                #queue_delay,runtime,post_script_delay,stage_in_delay,stage_out_delay\n",
    "                if len(features[features.index.str.startswith(new_l)])<1:\n",
    "                    print(f)\n",
    "                    print(new_l)\n",
    "                    continue\n",
    "\n",
    "                job_features = features[features.index.str.startswith(new_l)][['type', 'ready',\n",
    "                                       'submit', 'execute_start', 'execute_end', 'post_script_start',\n",
    "                                       'post_script_end', 'wms_delay', 'pre_script_delay', 'queue_delay',\n",
    "                                       'runtime', 'post_script_delay', 'stage_in_delay', 'stage_out_delay']].values.tolist()[0]\n",
    "                if job_features[0]=='auxiliary':\n",
    "                    job_features[0]= 0\n",
    "                if job_features[0]=='compute':\n",
    "                    job_features[0]= 1\n",
    "                if job_features[0]=='transfer':\n",
    "                    job_features[0]= 2\n",
    "                    #             print(job_features)\n",
    "                job_features = [-1 if x != x else x for x in job_features]\n",
    "                graph['x'].insert(lookup[l], job_features)\n",
    "            t_list=[]\n",
    "            for i in range(len(graph['x'])):\n",
    "                t_list.append(graph['x'][i][1])\n",
    "            minim= min(t_list)\n",
    "            for i in range(len(graph['x'])):\n",
    "                lim = graph['x'][i][1:7]\n",
    "                lim=[ v-minim for v in lim]\n",
    "                graph['x'][i][1:7]= lim\n",
    "            graphs.append(graph) \n",
    "    return graphs\n",
    "\n",
    "graphs= load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e0c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graph_all_.pkl','wb') as f:\n",
    "            pickle.dump(graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25951d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import json\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff70e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_gpu_available = torch.cuda.is_available()\n",
    "n_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b129cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "(array([0, 1, 2, 3]), array([1271, 1440, 1440, 2003]))\n",
      "Number of training graphs: 2000\n",
      "Number of test graphs: 1231\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ChebConv, SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = SAGEConv(14, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 4)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# flag = 'genome'\n",
    "# data_read=1\n",
    "# print(flag)\n",
    "# if data_read ==0:\n",
    "#     graphs = load_data(flag)\n",
    "#     ## Dumped into the pickle file.\n",
    "#     import pickle\n",
    "#     with open('graph_all_'+str(flag)+'.pkl','wb') as f:\n",
    "#         pickle.dump(graphs, f)\n",
    "# else:\n",
    "#     import pickle\n",
    "#     with open('graph_all_'+str(flag)+'.pkl','rb') as f:\n",
    "#         graphs = pickle.load(f)\n",
    "\n",
    "import pickle\n",
    "with open('graph_all_.pkl','rb') as f:\n",
    "    graphs = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "y_list = []\n",
    "for gr in graphs:\n",
    "    y_list.append(gr['y'])\n",
    "print(min(y_list))\n",
    "print(max(y_list))\n",
    "print(np.unique(np.array(y_list), return_counts=True))  \n",
    "datasets=[]\n",
    "import numpy\n",
    "for element in graphs:\n",
    "    gx = torch.tensor(numpy.array(element['x']) ) \n",
    "    ge =torch.tensor(numpy.array(element['edge_index']) ).T\n",
    "    gy =torch.tensor(numpy.array(element['y']).reshape([-1]))\n",
    "    #print(gx.shape, ge.shape, gy.shape)\n",
    "    # print(gy)\n",
    "    v_min, v_max = gx.min(), gx.max()\n",
    "    new_min, new_max = -1, 1\n",
    "    gx = (gx - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    # print(gx.min(), gx.max())\n",
    "    datasets.append( Data(x=gx, edge_index=ge, y=gy) )\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "dataset = datasets\n",
    "torch.manual_seed(12345)\n",
    "import random\n",
    "random.seed(12345)\n",
    "random.shuffle(datasets)\n",
    "train_dataset = datasets[: int(len(datasets)*0.80) ]\n",
    "test_dataset = datasets[int(len(datasets)*0.80):]\n",
    "random.shuffle(train_dataset)\n",
    "train_dataset= train_dataset[0:2000]\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2812f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Javascript\n",
    "# display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))     \n",
    "def train(model, criterion, optimizer, loader):\n",
    "    model.train()\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(data.x.float(), data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        \n",
    "\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x.float(), data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_run(train_ratio=0.95):\n",
    "    def run(config: dict):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        from torch_geometric.loader import DataLoader\n",
    "        train_loader = DataLoader(train_dataset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=int(config[\"batch_size\"]), shuffle=False)\n",
    "\n",
    "        model = GCN(hidden_channels=int(config[\"hidden\"]) ).float()\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "        import torch.optim.lr_scheduler as lrs\n",
    "        # scheduler = lrs.ExponentialLR(optimizer, gamma=0.9)\n",
    "        for _ in range(1, int(config[\"num_epochs\"]) + 1):\n",
    "            train(model, criterion, optimizer, train_loader)\n",
    "        accu_test = test(model, test_loader)\n",
    "        return accu_test\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f3131a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_run = get_run(train_ratio=0.3)\n",
    "perf_run = get_run(train_ratio=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "198136e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 16:02:59,509\tINFO services.py:1374 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Default Configuration:  0.315\n"
     ]
    }
   ],
   "source": [
    "# We define a dictionnary for the default values\n",
    "default_config = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"hidden\":64    \n",
    "}\n",
    "\n",
    "# We launch the Ray run-time and execute the `run` function\n",
    "# with the default configuration\n",
    "\n",
    "if is_gpu_available:\n",
    "    if not(ray.is_initialized()):\n",
    "        ray.init(num_cpus=n_gpus, num_gpus=n_gpus, log_to_driver=False)\n",
    "\n",
    "    run_default = ray.remote(num_cpus=1, num_gpus=1)(perf_run)\n",
    "    objective_default = ray.get(run_default.remote(default_config))\n",
    "else:\n",
    "    if not(ray.is_initialized()):\n",
    "        ray.init(num_cpus=1, log_to_driver=False)\n",
    "    run_default = perf_run\n",
    "    objective_default = run_default(default_config)\n",
    "\n",
    "print(f\"Accuracy Default Configuration:  {objective_default:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f66fde29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration space object:\n",
       "  Hyperparameters:\n",
       "    batch_size, Type: UniformInteger, Range: [8, 128], Default: 32, on log-scale\n",
       "    hidden, Type: UniformInteger, Range: [8, 128], Default: 32, on log-scale\n",
       "    learning_rate, Type: UniformFloat, Range: [0.001, 0.1], Default: 0.01, on log-scale\n",
       "    num_epochs, Type: UniformInteger, Range: [10, 100], Default: 55\n",
       "\n",
       "\n",
       "  Starting Point:\n",
       "{0: {'batch_size': 64, 'hidden': 64, 'learning_rate': 0.001, 'num_epochs': 10}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deephyper.problem import HpProblem\n",
    "\n",
    "problem = HpProblem()\n",
    "# Discrete hyperparameter (sampled with uniform prior)\n",
    "problem.add_hyperparameter((10, 100), \"num_epochs\")\n",
    "# Discrete and Real hyperparameters (sampled with log-uniform)\n",
    "problem.add_hyperparameter((8, 128, \"log-uniform\"), \"batch_size\")\n",
    "problem.add_hyperparameter((8, 128, \"log-uniform\"), \"hidden\")\n",
    "problem.add_hyperparameter((0.001, 0.1, \"log-uniform\"), \"learning_rate\")\n",
    "# problem.add_hyperparameter((0.00001, 0.9999, \"log-uniform\"), \"gamma\")\n",
    "# Add a starting point to try first\n",
    "problem.add_starting_point(**default_config)\n",
    "problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3efbc166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new evaluator with 1 worker and config: {'num_cpus': 1, 'num_cpus_per_task': 1, 'callbacks': [<deephyper.evaluator.callback.LoggerCallback object at 0x7fdcc49de9d0>]}\n"
     ]
    }
   ],
   "source": [
    "from deephyper.evaluator import Evaluator\n",
    "from deephyper.evaluator.callback import LoggerCallback\n",
    "\n",
    "def get_evaluator(run_function):\n",
    "    # Default arguments for Ray: 1 worker and 1 worker per evaluation\n",
    "    method_kwargs = {\n",
    "        \"num_cpus\": 1,\n",
    "        \"num_cpus_per_task\": 1,\n",
    "        \"callbacks\": [LoggerCallback()]\n",
    "    }\n",
    "\n",
    "    # If GPU devices are detected then it will create 'n_gpus' workers\n",
    "    # and use 1 worker for each evaluation\n",
    "    if is_gpu_available:\n",
    "        method_kwargs[\"num_cpus\"] = n_gpus\n",
    "        method_kwargs[\"num_gpus\"] = n_gpus\n",
    "        method_kwargs[\"num_cpus_per_task\"] = 1\n",
    "        method_kwargs[\"num_gpus_per_task\"] = 1\n",
    "\n",
    "    evaluator = Evaluator.create(\n",
    "        run_function,\n",
    "        method=\"ray\",\n",
    "        method_kwargs=method_kwargs\n",
    "    )\n",
    "    print(f\"Created new evaluator with {evaluator.num_workers} worker{'s' if evaluator.num_workers > 1 else ''} and config: {method_kwargs}\", )\n",
    "    return evaluator\n",
    "\n",
    "evaluator_1 = get_evaluator(quick_run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "451e7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.search.hps import AMBS\n",
    "# Uncomment the following line to show the arguments of AMBS.\n",
    "AMBS\n",
    "# Instanciate the search with the problem and a specific evaluator\n",
    "search = AMBS(problem, evaluator_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af1f6d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00001] -- best objective: 0.31519 -- received objective: 0.31519\n",
      "[00002] -- best objective: 0.31519 -- received objective: 0.31519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n",
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00003] -- best objective: 0.31519 -- received objective: 0.30950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n",
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00004] -- best objective: 0.38911 -- received objective: 0.38911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n",
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00005] -- best objective: 0.47929 -- received objective: 0.47929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n",
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00006] -- best objective: 0.47929 -- received objective: 0.30950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n",
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00007] -- best objective: 0.47929 -- received objective: 0.37124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n",
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00008] -- best objective: 0.47929 -- received objective: 0.30950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n",
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00009] -- best objective: 0.47929 -- received objective: 0.23152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n",
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00010] -- best objective: 0.47929 -- received objective: 0.46304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n",
      "/Users/krishnanraghavan/opt/anaconda3/envs/dh_posei/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:484: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_samples = df_samples.append(df_merge)\n"
     ]
    }
   ],
   "source": [
    "results = search.search(max_evals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6655f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deephyper.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc13279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
